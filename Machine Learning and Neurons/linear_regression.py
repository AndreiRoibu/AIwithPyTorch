# -*- coding: utf-8 -*-
"""Linear_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/154CXhkgz--XVuKrGLFkiNi-S1Iv1GdCU
"""

# We start by doing the required imports

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

N = 100 # Number of data points
X = np.random.random(N) * 10 - 5 # Random data in range [-5, 5]
y = 0.5 * X - 1 + np.random.randn(N) # A line plus noise

print(X.shape)
# True slope = 0.5
# True line intercept = -1
# Gaussia noise with mean=0 and variance=1 is added as this is directly linked with MSE as a loss function

plt.scatter(X, y) # Visualise the data

# Create the linear regression model 
# The model has 1 input and 1 output
model = nn.Linear(1,1)

# Define the loss and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr= 0.05)

# Reshape the data and define the inputs
X = X.reshape(N,1) # Number of samples x Number of Dimensions
y = y.reshape(N,1)

print(X.shape)

inputs = torch.from_numpy(X.astype(np.float32))
targets = torch.from_numpy(y.astype(np.float32))

# Now we train the model
number_epochs = 30
losses = []
for iteration in range(number_epochs):
    optimizer.zero_grad() # zero the parameter gradients. PyTorch accumulates the gradients for every .backward() call
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    losses.append(loss.item())
    loss.backward()
    optimizer.step()
    print('Epoch {}/{}, Loss: {:.6f}'.format(iteration+1, number_epochs, loss.item()))

# Plot the losses
plt.plot(losses)

# Plot the graph of predictions
predicted = model(inputs).detach().numpy()
plt.scatter(X, y, label='Original Data')
plt.plot(X, predicted, label='Fitted Line', color='r', linestyle='--')
plt.legend()
plt.show()

# Test the model values (true w = 0.5, b = -1)
w = model.weight.data.numpy()
b = model.bias.data.numpy()
print(w,b)

