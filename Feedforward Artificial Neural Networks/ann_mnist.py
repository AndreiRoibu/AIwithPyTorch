# -*- coding: utf-8 -*-
"""ANN MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yoV6PAVFCIgVeQXktUQHhyTJYuMnfVZs
"""

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import itertools

# Load the train data
train_dataset = torchvision.datasets.MNIST(
    root= '.',
    train= True,
    transform= transforms.ToTensor(),
    download= True
)

print("DataMax: {}".format(train_dataset.data.max()))
print("DataShape: {}".format(train_dataset.data.shape))
print("Targets: {}".format(train_dataset.targets))

# Load the test data
test_dataset = torchvision.datasets.MNIST(
    root='.',
    train=False,
    transform=transforms.ToTensor(),
    download= True
)

# Build the model
model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
model.to(device)

# Loss and optimizer
Loss = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

# Define the dataloader  
batch_size = 128
train_loader = torch.utils.data.DataLoader(
    dataset = train_dataset,
    batch_size = batch_size,
    shuffle= True
)

test_loader = torch.utils.data.DataLoader(
    dataset= test_dataset,
    batch_size= batch_size,
    shuffle = False
)

train_dataset.transform(train_dataset.data.numpy()).max()

# Here we train the model

number_epochs = 10
# Generate the empty loss storage in memory
train_losses = np.zeros(number_epochs)
test_losses = np.zeros(number_epochs)

for iteration in range(number_epochs):
    train_loss = []
    for inputs, targets in train_loader:
        inputs = inputs.to(device) 
        targets = targets.to(device)

        inputs = inputs.view(-1, 784) # we reshape the inputs
        
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = Loss(outputs, targets)

        loss.backward()
        optimizer.step()

        train_loss.append(loss.item())

    train_loss = np.mean(train_loss)

    test_loss = []
    for inputs, targets in test_loader:
        inputs = inputs.to(device) 
        targets = targets.to(device)

        inputs = inputs.view(-1, 784) # we reshape the inputs
        
        outputs = model(inputs)
        loss = Loss(outputs, targets)

        test_loss.append(loss.item())
    test_loss = np.mean(test_loss)

    train_losses[iteration] = train_loss
    test_losses[iteration] = test_loss

    print("Epoch: {}/{}, Train Loss: {}, Test Loss: {}".format(iteration+1, number_epochs, train_loss, test_loss))

# Plot the train & test losses

plt.plot(train_losses, label='train loss')
plt.plot(test_losses, label='test loss')
plt.legend()
plt.show()

# Calculate the train & test 

correct_train = 0
all_train = 0

for inputs, targets in train_loader:
    inputs = inputs.to(device) 
    targets = targets.to(device)

    inputs = inputs.view(-1, 784) # we reshape the inputs
    
    outputs = model(inputs)

    _, predictions = torch.max(outputs, 1) # returns the max and argmax

    correct_train += (predictions == targets).sum().item()
    all_train += targets.shape[0]

training_accuracy = correct_train / all_train

correct_test = 0
all_test = 0

test_loss = []
for inputs, targets in test_loader:
    inputs = inputs.to(device) 
    targets = targets.to(device)

    inputs = inputs.view(-1, 784) # we reshape the inputs
    
    outputs = model(inputs)
    _, predictions = torch.max(outputs, 1) # returns the max and argmax

    correct_test += (predictions == targets).sum().item()
    all_test += targets.shape[0]

testing_accuracy = correct_test / all_test

print('Accuracy for Train = {}, Test = {}'.format(training_accuracy, testing_accuracy))

# Calculate and Plot the Confusion Matrix

def plot_confusion_matrix(confusion_matrix, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):
    """Conf Matrix Function
    This function prints the confusion matrix
    Args:...
    """

    if normalize:
        confusion_matrix = confusion_matrix.astype('float') / confusionmatrix.sum(axis=1)[:, np.newaxis]
        print('Confusion Matrix Normalised!')
    else:
        print('Confusion Matrix has not been normalised...')

    print(confusion_matrix)

    plt.imshow(confusion_matrix, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)
    frmt = '.2f' if normalize else 'd'
    threshold = confusion_matrix.max() / 2.0
    for i, j in itertools.product(range(confusion_matrix.shape[0]), range(confusion_matrix.shape[1])):
        plt.text(j, i, format(confusion_matrix[i,j], frmt), horizontalalignment="center", color="white" if confusion_matrix[i,j] > threshold else "black")

    plt.tight_layout()
    plt.ylabel('True')
    plt.xlabel('Predicted')
    plt.show()

X_test = test_dataset.data.numpy()
y_test = test_dataset.targets.numpy()
p_test = np.array([])
for inputs, targets in test_loader:
    inputs = inputs.to(device) 
    targets = targets.to(device)

    inputs = inputs.view(-1, 784) # we reshape the inputs
    
    outputs = model(inputs)

    _, predictions = torch.max(outputs, 1) # returns the max and argmax

    p_test = np.concatenate((p_test, predictions.cpu().numpy()))

cm = confusion_matrix(y_test, p_test)
plot_confusion_matrix(cm, list(range(10)))

# Display missclassified examples

missclassified_idx = np.where(p_test != y_test)[0]
i = np.random.choice(missclassified_idx)
plt.imshow(X_test[i], cmap='gray')
plt.title('True Label: {}, Predicted: {}'.format(y_test[i], int(p_test[i])))

